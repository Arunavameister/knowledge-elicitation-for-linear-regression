1. Remove features that are all zero in the training data (in real data case scripts of main and main_all).
2. Add rng seeding somewhere so that we get reproducible results in all cases where randomness is used.
3. user feedback in the real data case should be created using a seperate set of data (not training or test). DONE
4. Check the normalization of X_all and Y_all in all the main scripts and use main_amazon as model for them.
5. Re-visit candi scripts and update them based on new edits.
6. Add non-sequential methods in the decision_policy function for better comparison.
7. Add comparisons to previous non-sparse methods we considered.
8. Other product category (for instance, amazon books reviews). 
9. Other datasets (still sparse, still easy to interpret). Some possibilities: hotel ratings (Tripadvisor), Rate-My-Professor.
10. Usinf the tf-idf instead of the count matrix. Then again removing some of the features. Does the performance of our method improve?


